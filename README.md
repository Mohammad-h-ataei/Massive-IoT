# Massive IoT Data Processing

This repository contains the code and configurations for a Massive IoT data pipeline, designed to handle high-throughput data ingestion, processing, and visualization.

## Project Overview

This project leverages Apache Kafka, Python, and Docker to build a scalable and real-time IoT data pipeline. It supports producer and consumer APIs for streaming data, ETL processing, and data visualization.

## Features

- **Kafka Producers and Consumers**: Handles data ingestion and streaming.
- **ETL with KsqlDB**: Processes data for analytics.
- **Dockerized Deployment**: Uses Docker Compose for environment setup.
- **Data Visualization**: Visualize data trends in real-time.

## Repository Structure

- **producer/**: Contains producer scripts for data ingestion.
- **consumer/**: Includes consumer scripts for data processing.
- **data/**: Sample data for testing.
- **docker-compose.yml**: Docker configuration for running the pipeline.

## Getting Started

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/Mohammad-h-ataei/Massive-IoT.git

2.  **Run with Docker**:
   ```bash

   docker-compose up


## Technologies Used
- **Apache Kafka** for data streaming  
- **Python** for data processing  
- **Docker** for containerization  
- **HTML** for simple visualizations
